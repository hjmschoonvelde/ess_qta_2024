---
title: "Supervised machine learning"
output:
  github_document:
  html_document:
    theme: readable
  pdf_document: default
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = TRUE)
```

This document walks you through an example of supervised machine learning to predict which UK prime minister delivered a speech. For this we'll use UK prime minister speeches from the [EUSpeech](https://dataverse.harvard.edu/dataverse/euspeech) dataset. 

For the supervised machine learning exercise you will need to install the `quanteda.textmodels`, `quanteda.textplots` and the `quanteda.textstat` libraries We will also use the `tidyverse` library to create training and test sets. Furthermore, we will use the `caret` library to produce a confusion matrix. This library requires some dependencies (i.e., functions from other libraries), so if you are working from your computer will need install it like so: `install.packages('caret', dependencies = TRUE)`.

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

#load libraries
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(tidyverse)
library(caret)

#read in speeches
speeches <- read.csv(file = "speeches_uk.csv", 
                     header = TRUE, 
                     stringsAsFactors = FALSE, 
                     sep = ",", 
                     encoding = "UTF-8")

#construct a corpus
corpus_pm <- corpus(speeches)
#select speeches from Cameron and Brown
corpus_brown_cameron <- corpus_subset(corpus_pm, speaker != "T. Blair")
#turn the date variable in a date format instead of character format
docvars(corpus_brown_cameron, "date") <- as.Date(docvars(corpus_brown_cameron, "date"), "%d-%m-%Y")
```

Let's tokenise this corpus and create a dfm

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}
tokens_brown_cameron <- tokens(corpus_brown_cameron,
                            what = "word",
                            remove_punct = TRUE, 
                            remove_symbols = TRUE, 
                            remove_numbers = TRUE,
                            remove_url = TRUE,
                            remove_separators = TRUE,
                            split_hyphens = FALSE,
                            padding = FALSE
                            ) %>%
  tokens_remove(stopwords("en"))

#to make this dfm less sparse, we will only select features that appear in at least 2% of speeches
dfm_brown_cameron <- dfm(tokens_brown_cameron ) %>%
  dfm_trim(min_docfreq = 0.02, docfreq_type = "prop")
    

dim(dfm_brown_cameron)
```

We now have a dfm containing 776 speeches delivered by either Gordon Brown or David Cameron and 4375 tokens. 

## Naive Bayes

Let's see if we can build a classifier to predict if a speech is delivered by Cameron or Brown. First, we'll generate a vector of 250 random numbers selected from the vector 1:776. We'll also append an id variable`id_numeric` to our dfm. **NB**: The `set.seed()` function makes sure that you can reproduce your random samples. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

#set.seed() allows us to reproduce randomly generated results 
set.seed(2)

#generate a sample of 250 numbers without replacement
id_train <- sample(1:nrow(dfm_brown_cameron), 250, replace = FALSE)
head(id_train, 10)

#create id variable
docvars(dfm_brown_cameron, "id_numeric") <- 1:ndoc(dfm_brown_cameron)

#take note of how many speeches were delivered by either Brown or Cameron
table(docvars(dfm_brown_cameron, "speaker"))
```

We then take a sample of 250 speeches as our training data and turn it into a dfm. The `%in%` operator produces a logical vector of the same length as id_numeric, and contains a TRUE if `id_numeric[i]` appears in id_train and FALSE otherwise. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

# create a training set: a dfm of 250 documents with row numbers included in id_train
train_dfm <- dfm_subset(dfm_brown_cameron, id_numeric %in% id_train)

#create a test set: a dfm of 100 documents whose row numbers are *not* included in id_train
test_dfm <- dfm_subset(dfm_brown_cameron, !id_numeric %in% id_train)
test_dfm <- dfm_sample(test_dfm, 100, replace = FALSE)

#check whether there is no overlap between the train set and the test set
which((docvars(train_dfm, "id_numeric")  %in% docvars(test_dfm, "id_numeric")))
```

We can now train a Naive Bayes classifier on the training set using the `textmodel_nb()` function

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

speaker_classifier_nb <- textmodel_nb(train_dfm, 
                                      y = docvars(train_dfm, "speaker"), 
                                      smooth = 1,
                                      prior = "docfreq",
                                      distribution = "multinomial")

summary(speaker_classifier_nb)


```

Let's analyze if we can predict whether a speech in the test set is from Cameron or Brown:

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

#Naive Bayes can only take features that occur both in the training set and the test set. We can make the features identical by passing train_dfm to dfm_match() as a pattern.

matched_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))

#predict speaker 
pred_speaker_classifier_nb <- predict(speaker_classifier_nb, 
                                      newdata = matched_dfm, 
                                      type = "class")

head(pred_speaker_classifier_nb)

```

We could also predict the probability the model assigns to each class. We could use this to create an ROC curve.


```{r, echo = TRUE, results = 'verbatim', message = FALSE}

#predict probability of speaker
pred_prob_classifier_nb <- predict(speaker_classifier_nb, 
                                   newdata = matched_dfm, 
                                   type = "probability")
```

Let's see how well our classifier did by producing a confusion matrix

```{r, echo = TRUE, results = 'verbatim', message = FALSE}
tab_class_nb <- table(predicted_speaker_nb = pred_speaker_classifier_nb, 
                      actual_speaker = docvars(test_dfm, "speaker"))

print(tab_class_nb)
```

So it appears we are somewhat successful at predicting whether a speech is delivered by Cameron or Brown. Our accuracy is 90%. 

Let's have a look at the most predictive features for Cameron in the complete corpus.

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

dfm_brown_cameron_grouped <- tokens_brown_cameron %>%
  tokens_group(groups = speaker) %>%
  dfm()

keyness <- textstat_keyness(dfm_brown_cameron_grouped, target = "D. Cameron")
textplot_keyness(keyness)


```

We can also display the most occurring features for Brown and Cameron but without account for their `keyness`:

```{r, echo = TRUE, results = 'verbatim', message = FALSE}
topfeatures(dfm_brown_cameron, groups = speaker, n = 50)
```

As you can see, it seems predictive features mostly indicate different topics.  


In order to improve on our predictions, we may think of other ways to represent our documents. A common approach is to produce a feature-weighted dfm by calculating the term-frequency-inverse document frequency (tfidf) for each token. The intuition behind this transformation is that it gives a higher weight to tokens that occur often in a particular document but not much in other documents, comppared to tokens that occur often in a particular document but also in other documents. Tf-idf weighting is done through `dfm_tfidf()`. 


```{r, echo = TRUE, results = 'verbatim', message = FALSE}

train_dfm_weighted <- dfm_tfidf(train_dfm)
matched_dfm_weighted <- dfm_tfidf(matched_dfm)

speaker_classifier_weighted_nb <- textmodel_nb(train_dfm_weighted, 
                                               y = docvars(train_dfm_weighted, "speaker"), 
                                               smooth = 1, #laplace smoothing as discussed in the lecture
                                               prior = "docfreq",
                                               distribution = "multinomial")

pred_speaker_classifier_weigthed_nb <- predict(speaker_classifier_weighted_nb, 
                                               newdata = matched_dfm_weighted, 
                                               type = "class")

tab_class_weighted_nb <- table(predicted_speaker_nb = pred_speaker_classifier_weigthed_nb, 
                               actual_speaker = docvars(test_dfm, "speaker"))

print(tab_class_weighted_nb)
```

We'll, we actually did a bit worse this time.  

## Logistic regression

Let's try a different classifier, a logistic regression. This regression is not any different a logistic regression you may have come across estimating a regression model for a binary dependent variable, but this time we use it solely for prediction

```{r, echo = TRUE, results = 'verbatim', message = FALSE}


speaker_classifier_lr <- textmodel_lr(train_dfm, 
                                      y = docvars(train_dfm, "speaker"))

#predict speaker 
pred_speaker_classifier_lr <- predict(speaker_classifier_lr, 
                                   newdata = matched_dfm, 
                                   type = "class")

#confusion matrix
tab_class_lr <- table(predicted_speaker_lr = pred_speaker_classifier_lr, 
                       actual_speaker = docvars(test_dfm, "speaker"))
print(tab_class_lr)
```

We now need to decide which of these classifiers works best for us. As discussed in class we would need to check precision, recall and F1 scores. The `confusionMatrix()` function in the `caret` package does exactly that. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}
confusionMatrix(tab_class_nb, mode = "prec_recall")

confusionMatrix(tab_class_lr, mode = "prec_recall")
```

Based on the F1 score, our logistic regression classifier is performing slightly better than predictions from our Naive Bayes classifier, so, all else equal we would go with logistic regression. 


## Exercises

For this set of exercises we will use the `data_corpus_moviereviews` corpus that is stored in **quanteda**. This dataset contains 2000 move reviews which are labeled as positive or negative. We'll try to predict these labels using supervised machine learning. 

Apply `table()` to the sentiment variable appended to `data_corpus_moviereviews` to inspect how many reviews are labelled positive and negative.

```{r}
table(docvars(data_corpus_moviereviews, "sentiment"))
```

Check the distribution of the number of tokens across reviews by applying `ntoken()` to the corpus and then produce a histogram using `hist()` .  
```{r}


data_corpus_moviereviews %>% 
    ntoken() %>% 
    hist()

```


Tokenise the corpus and save it as `tokens_reviews`. 

```{r}
tokens_reviews <- tokens(data_corpus_moviereviews,
                         what = "word",
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_numbers = TRUE,
                         remove_url = TRUE,
                         remove_separators = TRUE,
                         split_hyphens = FALSE,
                         padding = FALSE) %>%
  tokens_remove(stopwords("en"))
```

Create a document-feature matrix and call it `dfm_reviews`

```{r}
dfm_reviews <- dfm(tokens_reviews)
```

Apply `topfeatures()` to this dfm and get the 50 most frequent features.

```{r}
topfeatures(dfm_reviews, n = 50)
```

**Optional** Repeat this step, but get the 25 most frequent features from reviews labelled as "pos" and "neg".


```{r}

topfeatures(dfm_reviews, groups = sentiment, n = 25)
```


## Supervised Machine Learning

Set a seed (`set.seed()`) to ensure reproducibility. 

```{r}
set.seed(123)
```

Create a new dfm with a random sample of 1500 reviews. We will use this dfm as a training set. Call it `train_movies_dfm`. Use the sampling code we used above. 


```{r}
docvars(dfm_reviews, "id") <- 1:ndoc(data_corpus_moviereviews)
id_train <- sample(1:ndoc(data_corpus_moviereviews), 1500, replace = FALSE)

train_movies_dfm <- dfm_subset(dfm_reviews, id %in% id_train)

```

Create another dfm with the remaining 500 reviews. Call it `test_movies_dfm`. This will be our test set.


```{r}

test_movies_dfm <- dfm_subset(dfm_reviews, !id %in% id_train)

```

Apply `textmodel_nb()` to the dfm consisting of 1500 documents. Use "sentiment" to train the classifier. Call the output `tmod_nb`

```{r}
tmod_nb <- textmodel_nb(train_movies_dfm, 
                        y =train_movies_dfm$sentiment, 
                        smooth = 1,
                        prior = "docfreq",
                        distribution = "multinomial")
```

Predict the sentiment of the remaining 500 documents by using `predict()`. Call the output `prediction`.


```{r}
prediction <- predict(tmod_nb, newdata = test_movies_dfm, type = "class")
```

Create a cross-table/confusion matrix to assess the classification performance using `table()`. 

```{r}
tab_nb <- table(prediction = prediction, 
                human = docvars(test_movies_dfm, "sentiment"))

print(tab_nb)

```


