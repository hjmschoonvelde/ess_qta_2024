---
title: "QTA Day 9: Word embeddings"
output:
  github_document:
  html_document:
    theme: readable
  pdf_document: default
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The goal of today's lab session is to develop an understanding for word embeddings. We'll train a word embeddings model using the **text2vec** library (Selivanov, Bickel & Wang, 2022) on a set of speeches of European Commissioners and we'll inspect these embeddings.

NB: Keep in mind that this lab session is meant for practice purposes only. The word vectors that we'll inspect require careful validation. 

Let's load the required libraries first.

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

library(quanteda)
library(quanteda.textstats)
library(tidyverse)
library(text2vec)

```


## Preparing the data

Let's read in the Commission speeches

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

load("european_commission.Rdata")

dim(commission_speeches)
names(commission_speeches)
```

We'll tokenise the speeches.  

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

corpus_speeches <- corpus(commission_speeches)
tokens_speeches <- tokens(corpus_speeches,
                          what = "word",
                          remove_punct = TRUE, 
                          remove_symbols = TRUE, 
                          remove_numbers = TRUE,
                          remove_url = TRUE,
                          remove_separators = TRUE,
                          split_hyphens = FALSE,
                          ) %>%
  tokens_remove(stopwords(source = "smart"), padding = FALSE)


```


NB: The next few steps draw on [this](https://quanteda.io/articles/pkgdown/replication/text2vec.html) **quanteda** tutorial.

We'll select those features that occur at least 10 times 
```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

feats <- dfm(tokens_speeches) %>%
    dfm_trim(min_termfreq = 10) %>%
    featnames()
```

We'll now select these features from the tokenised speeches. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

tokens_speeches <- tokens_select(tokens_speeches, 
                                 feats)

```

Let's inspect which features occur most often

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

tokens_speeches %>%
  dfm() %>%
  topfeatures(n = 100,
              decreasing = TRUE,
              scheme = c("count")
)
  

```
We'll create a feature-co-occurrence matrix using the `fcm()` function which calculates co-occurrences of features within a user-defined context. We'll choose a window size of 5, but other choices are available

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

speeches_fcm <- fcm(tokens_speeches, 
                    context = "window", 
                    window = 5,
                    tri = TRUE)

dim(speeches_fcm)
```

Let's see what `speeches_fcm()` looks like.

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

speeches_fcm[1:5,1:5]
```

_Dear_ and _President_ co-occur 221 times in the corpus. _Dear_ and _Regions_ only 16 times. 

## Fitting a GloVe model

We'll now fit a GloVe vector model. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on the feature co-occurrence matrix,  which represents information about global word-word co-occurrence statistics in a corpus.

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

glove <- GlobalVectors$new(rank = 50, 
                           x_max = 10)

wv_main <- glove$fit_transform(speeches_fcm, 
                               n_iter = 10,
                               convergence_tol = 0.01, 
                               n_threads = 8)

dim(wv_main)

```

The model learns two sets of word vectors - main and context. They are essentially the same. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

wv_context <- glove$components
dim(wv_context)
```

Following recommendations in the **text2vec** package we sum these vectors. We transpose the `wv_context` object so that it has the same number of rows and columns as `wv_main`

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

word_vectors <- wv_main + t(wv_context)

dim(word_vectors)
```

We now have 50-dimension word_vectors for all 21516 tokens in our corpus.

## Inspecting the GloVe model

Now it's tme to inspect these word embeddings. For example, we find the nearest neighbors of a word (or a set of words) of interest. Nearest neighbors are those words that are most closely located in the vector space. We can find those using by calculating cosine similarities between the word vector of a target word and all other word vectors.  

We'll use a custom function ([source](https://s-ai-f.github.io/Natural-Language-Processing/Word-embeddings.html)) to finds these similar words It takes in three arguments: the target word, the word_vectors object, and the number of neighbors we want to inspect. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

find_similar_words <- function(word, word_vectors, n = 10) {
  similarities <- word_vectors[word, , drop = FALSE] %>%
    sim2(word_vectors, y = ., method = "cosine")
  
  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)
}

```

The Commissioner speeches span the time period 2007--2014, a time of upheaval in the EU. Let's take a look at the nearest neighbors of 'crisis'. The `drop = FALSE` argument ensure that crisis is not converted to a vector.


```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

find_similar_words("crisis", word_vectors)

```
Crisis refers mostly to the Eurocrisis. 

Let's inspect the context of climate

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

find_similar_words("climate", word_vectors)

```
Global climate change needs to be addressed, that much is clear. 

We can sum vectors to each find neigbors. Let's add crisis + Ireland

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

crisis_Ireland <- word_vectors["crisis", , drop = FALSE] +
  word_vectors["Ireland", , drop = FALSE] 

```

And find the nearest neighbors of this sum. The sim2 function calculates the cosine similarity between the word vectors of the target word and all other word vectors. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

cos_sim_crisis_Ireland <- sim2(x = word_vectors, y = crisis_Ireland, method = "cosine", norm = "l2")
head(sort(cos_sim_crisis_Ireland[,1], decreasing = TRUE), 20)

```
It mostly lists other countries that where also struggling at the time. 

What if we substract the Ireland vector from the crisis vector? 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

crisis_Ireland <- word_vectors["crisis", , drop = FALSE] -
  word_vectors["Ireland", , drop = FALSE] 

cos_sim_crisis_Ireland <- sim2(x = word_vectors, y = crisis_Ireland, method = "cosine", norm = "l2")
head(sort(cos_sim_crisis_Ireland[,1], decreasing = TRUE), 10)

```
This time we get more general crisis terms. 

Inspecting a word embeddings model like so can be useful for a few different tasks:

1. As a list of potential terms for dictionary construction;
2. As an input to downstream QTA tasks;
3. As a source for visualization.

Let's take this first task an example. Perhaps we want to develop a sentiment dictionary for Commissioner speeches, but we are less trusting of off-the-shelf sentiment dictionaries because we suspect that these may not capture how sentiment is expressed in Commissioner speeches. One way to go is use a small seed dictionary of positive and negative words, and use word embeddings to inspect what other words are close in the embedding space to these seed words.

For example, we may take as positive words a small set of positive seed words: _good_, _nice_, _excellent_, _positive_, _fortunate_, _correct_, _superior_. And as negative words a small set of negative seed words: _bad_, _nasty_, _poor_, _negative_, _wrong_, _unfortunate_

Let's start by calculating the average vector for good. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

positive <- (word_vectors["good", , drop = FALSE] +
  word_vectors["nice", , drop = FALSE] +
  word_vectors["excellent", , drop = FALSE] +
  word_vectors["positive", , drop = FALSE] + 
  word_vectors["fortunate", , drop = FALSE] + 
  word_vectors["correct", , drop = FALSE] + 
  word_vectors["superior", , drop = FALSE]) /7
```

And for bad

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

negative <- (word_vectors["bad", , drop = FALSE] +
  word_vectors["nasty", , drop = FALSE] +
  word_vectors["poor", , drop = FALSE] +
  word_vectors["negative", , drop = FALSE] + 
  word_vectors["wrong", , drop = FALSE] + 
  word_vectors["unfortunate", , drop = FALSE]) /6

```

We can now inspect the neighbors of our 'positive' seed dictionary. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

cos_sim_positive <- sim2(x = word_vectors, y = positive, method = "cosine", norm = "l2")
head(sort(cos_sim_positive[,1], decreasing = TRUE), 20)

```

This includes some words that seem useful such as encouraging and opportunity and forward. But also the word bad appears.  

Let's do the same for our 'negative' dictionary

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

cos_sim_negative <- sim2(x = word_vectors, y = negative, method = "cosine", norm = "l2")
head(sort(cos_sim_negative[,1], decreasing = TRUE), 20)

```

Again we see a mix of useful and less useful words. 

## Exercises


Estimate new word vectors but this time on a feature co-occurrence matrix with a window size of 5 but with more weight given to words when they appear closer to the target word (see the _count_ and _weight_ arguments in `fcm()`. To estimate this model comment out the code chunk below to run the model) 

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

#speeches_fcm_weighted <- fcm(tokens_speeches, 
#                    context = "window", 
#                    count = "weighted", 
#                    weights = 1 / (1:5),
#                    tri = TRUE)


#glove <- GlobalVectors$new(rank = 50, 
#                           x_max = 10)

#wv_main_weighted <- glove$fit_transform(speeches_fcm_weighted, 
#                                        n_iter = 10,
#                                        convergence_tol = 0.01, 
#                                        n_threads = 8)

#wv_context_weighted <- glove$components

#word_vectors_weighted <- wv_main_weighted + t(wv_context_weighted)

```

2. Compare the nearest neighbors for crisis in both the original and the new model. Are they any different?

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}

#find_similar_words("crisis", word_vectors)
#find_similar_words("crisis", word_vectors_weighted)
```
3. Inspect the nearest neighbors for Greece, Portugal, Spain and Italy and substract the vectors for Netherlands, Germany, Denmark and Austria

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}
#southern_northern  <- (word_vectors["Greece", , drop = FALSE] +
#  word_vectors["Portugal", , drop = FALSE] +
#  word_vectors["Spain", , drop = FALSE] +
#  word_vectors["Italy", , drop = FALSE] -
#  word_vectors["Netherlands", , drop = FALSE] -
#  word_vectors["Germany", , drop = FALSE] -
#  word_vectors["Denmark", , drop = FALSE] -
#  word_vectors["Austria", , drop = FALSE])


#cos_sim_southern_northern <- sim2(x = word_vectors, y = southern_northern, method = "cosine", norm = "l2")
#head(sort(cos_sim_southern_northern[,1], decreasing = TRUE), 20)

```

4. And turn this vector around

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}
#northern_southern  <- (word_vectors["Netherlands", , drop = FALSE] +
#  word_vectors["Germany", , drop = FALSE] +
# word_vectors["Denmark", , drop = FALSE] +
# word_vectors["Austria", , drop = FALSE] -
# word_vectors["Greece", , drop = FALSE] -
#  word_vectors["Portugal", , drop = FALSE] -
#  word_vectors["Spain", , drop = FALSE] -
#  word_vectors["Italy", , drop = FALSE])


#cos_sim_northern_southern <- sim2(x = word_vectors, y = northern_southern, method = "cosine", norm = "l2")
#head(sort(cos_sim_northern_southern[,1], decreasing = TRUE), 20)

```

5. Inspect these word vectors further. If you receive a `subscript out of bounds` error, it means that the word does not appear in the corpus.

```{r, echo = TRUE, results = 'verbatim', message = FALSE, warning = FALSE}
```