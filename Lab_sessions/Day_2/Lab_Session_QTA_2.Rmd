---
title: "QTA Day 2: String operations and inspecting a corpus"
output:
 # html_document:
  #theme: readable
    md_document:
     variant: markdown_github
#date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = TRUE)
```


## String operations 

`R` stores text as a `string` or `character` vector. It is important to become comfortable with string operations, since they allow you to clean a string before you start analyzing your text. 

These string operations require familiarity with regular expressions in `R` as well as with functions in the **stringr** library.^[`R` is open source with different developers working on similar issues, as a result of which there can be multiple packages that do the same thing. For example, functions in base `R` and the **stringi** package also let you manipulate strings in `R`. However, the syntax of the function calls in these different packages is different.] 

Packages such as **quanteda** include some string cleaning functions as well but knowledge of regular expressions and string operations allow you to deal much better with with the specifics of your text. That said, these operations usually involve lots of trial and error, Google, or conversations with ChatGPT to figure out how to do certain things. But they help you clean your data, which will save you lot of headaches later on. Let's have a look at a set of useful functions.

First load the `stringr` library in `R`.

```{r, out.width='\\textwidth', message=FALSE, warning=FALSE}

library(stringr)

```

Then create a string vector called shopping_list:

```{r, out.width='\\textwidth'}

shopping_list <- c("4 bananas", " 136 Apples", "20 oranges", "1 Milk", "2 eggs")

```

Vectors are basic objects in `R` which contain a set of values of the same type (character, numeric, factor, etc.) The shopping_list contains five character. Check that this is true with the `str()` function: 

```{r, out.width='\\textwidth'}

str(shopping_list)

```

The `stringr` library contains many useful functions for working with character values, which are listed in this [cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/strings.pdf). Let's go through a few examples, starting with the `str_extract()` function for basic pattern matching.

`str_extract()` takes in two arguments: a string and a pattern to look for. For each string it returns the pattern it found. Let's see if it finds a n in each of the strings:

```{r, out.width='\\textwidth'}

str_extract(shopping_list, "n")

```
We can use `str_which()` to find the indexes of strings that contain a pattern match:

```{r, out.width='\\textwidth'}

str_which(shopping_list, "n")

```

Using `str_locate()` we can find the position of the first match in each string:

```{r, out.width='\\textwidth'}

str_locate(shopping_list, "n")

```

Using `str_locate_all()` we can find the position of all matches in each string:

```{r, out.width='\\textwidth'}

str_locate_all(shopping_list, "n")

```


Functions in the `stringr` library can work with regular expressions to extract content from a string vector in a more systematic way. For example, run the following line of code:

```{r, out.width='\\textwidth'}

str_extract(shopping_list, "\\d")

```

Here `d` is a regular expression which refers to any number (**NB**: without the escape character `\\` it would just refer to the letter d). See what happens when you replace `d` with `d+` like so: `str_extract(shopping_list, "\\d+")`. What does the + do?

```{r, out.width='\\textwidth'}

str_extract(shopping_list, "\\d+")
#your answer here

```

Let's turn to alphabetic characters (aka letters). The regular expression `[a-z]` refers to any lower case letter. The `+` symbol after `[a-z]` refers to one or more lower case letters. The `{3,4}` refers to a range of 3 to 4 lower case letters. The regular expression `[A-z]` refers to any upper or lower case letter. The `\\b` refers to a word boundary. Let's see how these work in practice:


```{r, out.width='\\textwidth'}

#extract the first lower case charachter in each string
str_extract(shopping_list, "[a-z]")

#extract lower case characters one or more times (again note the "+" symbol after "[a-z]")
str_extract(shopping_list, "[a-z]+")

#extract up to four lower case letters occurring in a row
str_extract(shopping_list, "[a-z]{3,4}")

#extract up to four upper OR lower case letters
str_extract(shopping_list, "[A-z]{1,4}")

#extract all letters in each string
str_extract_all(shopping_list, "[A-z]+")

#extract all numbers in each string
str_extract_all(shopping_list, "\\d+")

```

Note that str_extract_all generates a list of character strings as output. This can be simplified into a character matrix using the simplify command:

```{r, out.width='\\textwidth'}

str_extract_all(shopping_list, "\\b[A-z]+\\b", 
                simplify = TRUE)

str_extract_all(shopping_list, "\\d", 
                simplify = TRUE)

```

Let's have a look at the `str_replace()` function, which replaces a pattern in a string with another pattern. This function takes in three arguments: _string, pattern and replacement_. The string is the text you want to modify, the pattern is the text you want to replace, and the replacement is the text you want to replace it with.

Let's replace the first vowel in each string with a dash. And then replace all vowels with a dash:

```{r, out.width='\\textwidth'}

#replace first vowel
str_replace(shopping_list, "[aeiou]", "-")

#replace all vowels
str_replace_all(shopping_list, "[aeiou]", "-")

```

In *R*, you write regular expressions as strings, sequences of characters surrounded by quotes ("") or single quotes (''). Characters like +, ?, ^, and . have a special meaning as regular expressions and cannot be represented directly in an R string (see the RegEx [cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/strings.pdf) for more examples). In order to match them literally, they need to be preceded by two backslashes: ``\\".

Let's start with an example of a vector of names that for some reason contain dots and plus signs.


```{r, out.width='\\textwidth'}

name_list <- c("Jo.hn", "Anna.", "Si.+si", "Ma.ria")

```

Compare the output of these two calls

```{r, out.width='\\textwidth'}

str_replace(name_list, ".", "-")
str_replace(name_list, "\\.", "-")

```

Ccompare the output of these two calls:

```{r, out.width='\\textwidth'}

str_replace(name_list, ".+", "-")
str_replace(name_list, "\\.\\+", "-")
```

## Inspecting a corpus

For the next part of this script we'll first need to load the **quanteda** package. **quanteda** is a package for the quantitative analysis of textual data. It is a powerful package that allows you to preprocess, analyze, and visualize text data.

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

library(quanteda)

```

**quanteda** has several in-built corpora we can use for exercises. One of these corpora is `data_corpus_inaugural` which contains the inaugural speeches of all the American presidents in a corpus format. Type `summary(data_corpus_inaugural)` and inspect the object. 

```{r}

summary(data_corpus_inaugural,  n = 10)

```

Let's make a copy of this corpus. We'll save it in our working environment as an object called `speeches_inaugural`

```{r}

speeches_inaugural <- data_corpus_inaugural

```

We can inspect the content of the first inaugural speech using `as.character()` function:


```{r}

as.character(speeches_inaugural)[1]

```

This produces the text of George Washington's first inaugural speech. Metadata such as year, speaker, etc. are stored in a corpus object as _docvars_, and can be accessed like so: 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

#year
docvars(speeches_inaugural, "Year")

#party
head(docvars(speeches_inaugural, "Party"), 10)

```

Using the `table()` function we can inspect the number of presidents of each party:

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

#number of presidents of each party
table(docvars(speeches_inaugural, "Party"))

```

We can also inspect the number of documents in the corpus using the `ndoc()` function:

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

ndoc(speeches_inaugural)

```

Subsetting a corpus is easy using the `corpus_subset()` function. Note the `==` operator here. In `R` `=` is primarily used for assignment within function calls. It assigns values to variables. It can also be used for variable assignment, but this usage is less common compared to the `<-` operator, which is the preferred assignment operator in R.
The `==` operator in `R` is used for comparison. It checks if two values are equal and returns a logical value (TRUE or FALSE).

Using the `==` operator, we can create an object that only contains the inaugural speech of Donald Trump and call it `trump_inaugural`

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

trump_inaugural <- corpus_subset(speeches_inaugural, President == "Trump")

ndoc(trump_inaugural)

```

As you can see, Trump only appears only one time in the corpus. That's because he got voted out of office in 2020, although it remains to be seen what happens in 2024. Let's inspect the content of his inaugural speech:

```{r, out.width='\\textwidth'}

as.character(trump_inaugural)

```

If the documents are clean enough (i.e., with correct interpunction etc.), then it is easy in **quanteda** to break down a document on a sentence to sentence basis using `corpus_reshape()`, which reshapes the corpus to a different level of granularity, in this case to the level of sentences.

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

trump_sentence <- corpus_reshape(trump_inaugural, to =  "sentences")

ndoc(trump_sentence)

```

As you can see, Trump's inaugural speech consisted of 88 sentences.

Before we preprocess our texts, we first need to tokenize it using `tokens()`. Tokenization is the process of breaking a text into smaller units, such as words. 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

tokens_speeches_inaugural <- tokens(speeches_inaugural)

```

Using `tokens_compound()` we can create multiword expressions. Multiword expressions are phrases that consist of more than one word. Let's say we want to make certain that references to the `United States of America` are recognized as such in subsequent analyses. We can do so using the following line of code: 

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

tokens_speeches_inaugural <- tokens_compound(tokens_speeches_inaugural, phrase("United States of America"))

```

Let's see how often each President referred to the United States of America in their inaugural speeches:


```{r, echo = TRUE, results = 'verbatim', message = FALSE}

str_count(as.character(speeches_inaugural), "United States of America")

```
Apparently, this is a turn of phrase that is used more often in recent years, but not so much in the past.

Using the `kwic()` function, we can inspect the context in which the United States of America is used in these speeches. The `kwic()` function takes in a tokenized text and a pattern to look for. It returns a keyword-in-context (KWIC) object, which is a data frame that shows the context in which the pattern occurs. In this case we'll look at the context in which the United States of America of 10 words before and after the phrase:

```{r, echo = TRUE, results = 'verbatim', message = FALSE}

kwic(tokens_speeches_inaugural, 
     pattern = phrase("United_States_of_America"),
     window = 10)  %>%
  tail()

```

## Excercise: inspecting a corpus

For these exercises we'll use the `speeches_inaugural` corpus that we just created.

Explore `corpus_subset()` and filter only speeches delivered since 1990. Store this object as `speeches_inaug_since1990`. 

```{r}

#your answer here

```

Explore `corpus_reshape()`. Reshape `speeches_inaugural`to the level of sentences and store this object as `sentences_inaug_since1990`. What is the number of sentences (= number of documents) in this text corpus?

```{r}

#your answer here

```

Inspect how often references to the `pursuit of happiness` occur in this corpus:

```{r}


```

Tokenize the sentence-level text corpus. Make sure that references to the `Supreme Court` and `pursuit of happiness`  are included as multiword expressions.  
  

```{r}

#your answer here

```

Use corpus_reshape() and change the unit of `speeches_inaug_since1990` to the level of paragraphs. How many paragraphs does this corpus contain?

```{r}

#your answer here

```

## Excercise: cleaning a text vector

The local zoo has made an inventory of its animal stock.^[This exercise is based on an example from Automated Data Collection With R, by Munzert *et al* (2015).]  However, the zoo keepers did a messy job with writing up totals as you can see below. You are hired to clean up the mess using *R*.

```{r, out.width='\\textwidth'}

zoo <- c("bear x2", "Ostric7", "platypus x60", "x7 Eliphant", "x16 conDOR")

```

Use the functions in the `stringr` to clean up the string, taking out typos. Generate a dataframe with the following variables: *animal* (character), *number* (numeric). 

```{r, out.width='\\textwidth'}

#your answer here

```

Plot this data using `ggplot2`, call the resulting plot `zoo_plot`, and save it in a file called `zoo_plot.png` in the current working directory. 

```{r, out.width='\\textwidth'}

#your answer here

```




